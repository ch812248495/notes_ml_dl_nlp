# 批标准化
论文"Batch Normalization: Accelerating Deep Network Training by  Reducing Internal Covariate Shift"阅读笔记

## BN算法优势
- 可以设定比较大的学习率, 因为 BN 理论上避免了深度学习中梯度消失的存在, 所以训练很快
- dropout, L2等正则化方式和繁冗的超参数优化可以被完全省略, BN 能有极好的泛化能力, 而且能更好进行迁移学习
- 训练数据可以被打乱, 不会因为每个批次的数据分布截然不同而使得网络学习进度缓慢

## 背景知识
神经网络在对数据进行训练的时候, 一般会做一个简单的归一化, 至少会采用某些方程让输入数据分布不至于太过悬殊和离散, 减少离群点在训练中对网络的伤害, 比如
$log(1+x)$ 或白化(均值0, 方差1), 这是为了让离群点不至于太过离开均值附近, 从而使得学习分布比较容易, 不至于因为离群点吸引了太多注意而造成过拟合, 影响泛化能力.
实际上, 神经网络的本质, 就是学习训练数据的一个分布情况, 当前批梯度下降学习盛行的情况下, 如果每一批次的分布各不相同, 那么网络的泛化能力也就大大降低, 实际上做过机器学习的人都知道, 所谓的过拟合和欠拟合, 如果从外部找原因, 就是因为测试数据和训练数据的分布不太相同, 而模型强调了这种不同并倾向于训练数据而导致了偏差.
深度神经网络的训练是一个复杂的过程, 如果前面几层出现了微小的改变, 这样的改变将会在后面无限放大(特别是激活函数是 softmax, tanh 之类的, 容易过饱和). 如果输入的批数据发生改变, 那么参数的学习将会强制在这一轮次中, 向截然相反的方向优化, 大大降低了学习的速度. 前面层次的参数的更新, 将引起连锁反应, 后面的所有层也会随之改变, 这让模型对特定数据特别敏感, 不够健壮. 在训练过程中数据分布的改变(和之前学习到的分布完全不一样)称之为"Internal Covariate Shift", BN 的提出就是为了解决这个问题.

## 本质
让每个隐层节点的激活输入固定下来, 且这个值尽可能原理饱和区(尽量在线性区的和非线性区), 使得激活函数的输入落在对于输入比较敏感的区域, 避免梯度消失问题

## 白化层
从初始数据归一化得到启发, 为了减少数据偏离对初始层的影响对后面产生的蝴蝶效应, 我们在中间的每一层的输出后面加上一个白化层, 这就是 BN 的基本原理, 不过这个白化层是一个自我学习的, 有参数的层, 并没有那么简单
这里的白化就是最一般的白化, 将输入的数据分布归一化为 N(0,1), 公式如下<br>
$$\hat{x^{k}}=\frac{x^{k}-E[x^{k}]}{\sqrt{Var[x^{k}]}}$$
通过这个公式, 就能把每一层数据的分布从难以确定, 改变成严格的 N(0,1), 这里的数据范畴是当前批次

## 引入新参数
如果仅仅是对每一层的数据进行简单的白化处理, 是会破坏一些本来就应该具有明显偏向性分布的特征的, 所以在每层白化的基础上, 作者重构了白化得到的结果, 这也是这篇文章之所以能 stand out的原因:
$$y = a*\hat{x}^{k} + b^{k}$$
对比上面两个式子, 当 a, b 满足一定条件的时候, 白化后的数据是可以还原出原始数据的, a,b

## 源码实现
<code>

    m = K.mean(X, axis=-1, keepdims=True) #计算均值
    std = K.std(X, axis=-1, keepdims=True) #计算标准差
    X_normed = (X - m) / (std + self.epsilon) #归一化
    out = self.gamma * X_normed + self.beta #重构变换

</code>

## 前向传播
对于任一批次的数据, 均值就是批次的均值, 在没有采用 BN 的时候, 激活函数是这样的
$$y = g(Wx+b)$$
当采用了 BN 之后, 传播的形式变成了
$$y = g(BN(Wu+b))$$
实际上这个 b 在经过 BN 之后是没有用的, 所以最终形式为
$$y = g(BN(Wu))$$

## 反思和总结
- 为何要引入 a, b 两个参数?<br>
我们知道, 如果不进行新参数的引入, 那么相当于非线性函数的表达能力很大程度上丧失了(输入95%集中在(-2,+2)的区域), 基本是一个线性的表达能力, 而多层线性神经元组成的 DNN( 没有激活函数)是没有意义的, 所以增加两个参数, 一定程度上"还原"原输入数据的特征, 只不过在经过归一之后恢复(通过反向传播学习), 一定程度上去除了该批次数据特有的特征, 增强了泛化能力
这样做, 能让统一的正态分布长胖(瘦)一些, 向左(右)偏移一些, 不完全失去原有数据的特征. 核心思想是找一个线性和非线性(神经网络过拟合非线性的能力太强了)的平衡点, 既能享受非线性强表达能力, 又能享受线性区域快速的梯度学习能力, 也就是 softmax 比较居中的地域, 非常妙

## 最后的话
有人说 BN 有点类似于 Dropout 的防止过拟合的正则化表达方式, 个人对此存疑. 我认为这两个完全不相同, 一个是特征选择, 一个是中庸化分布.
